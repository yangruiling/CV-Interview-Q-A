# 五、深度学习框架-Pytorch代码

## PyTorch 的基本要素是什么？

-   PyTorch 张量
-   PyTorch NumPy
-   数学运算
-   Autograd 模块
-   优化模块
-   nn 模块

## Pytorch实现深度学习模型的标准流程

**Part1: 准备数据集（转后期笔记）**

生成数据模拟数据集，采用mini_batch策略【2.3节笔记】

**Part2:** 将模型定义为一个类，继承自**torch.nn.Module - 计算预测值y_pred（nn.Module的使用）**

核心：构造计算图，pytorch会自动计算梯度

内部至少定义两个函数：

-   **init（self）- 构造函数，在初始化对象时默认调用**super(当前class的名称, self).*init()*：定义父类的构造
    -   定义模型具体结构。eg：self.linear = torch.nn.Linear(1, 1) 构造一个线性层对象Unit（输入的特征维数,输出的特征维数,bias=True），包含Tensor类型的weight和bias
        -   self.linear(x): nn.linear类实现了_call_()-可调用对象，使得对象可以像函数一样被调用，会默认调用forward()
-   **forward（）-定义前馈运算**

    **Part3: 构造损失函数和优化器-梯度更新（Pytorch API）**

1.  **创建损失函数对象**：criterion = torch.nn.MSELoss(size_average=True, reduce=True) 继承自Module size_average：损失是否要求均值；reduce：损失是否要降维
2.  **创建优化器对象**：optimizer = torch.optim.SGD(model.parameters(), lr = 0.01, momentum = 0, dampening = 0, weight_decay = 0, neaterov = False) model.parameters(): 取出模型中所有需要优化的Tensor参数

    **Part4: 训练周期（前馈计算、反馈计算、更新梯度）**

3.  权重初始化
4.  调用model对象执行前馈计算
5.  计算损失
6.  优化器权重梯度归零
7.  反向传播
8.  优化器更新

## Pytorch实现线性模型

## Pytorch实现梯度下降

## Pytorch实现反向传播

## Pytorch实现线性回归

## Pytorch实现逻辑回归(二分类)

## Pytorch实现dataset与dataloader-实现数据的逐批次读取

## Pytorch实现softmax分类器（多分类-手写数字分类）

## Pytorch合理利用GPU-将model和数据放入GPU

## 手写数字分类数据集载入

## Pytorch实现基础CNN

## Pytorch实现Inception块

## Pytorch实现GoogleNet

## Pytorch实现Residual block和resNet

## Pytorch实现RNN(RNN和RNN cell两种方式)

## [卷积层前向与反向传播](https://blog.csdn.net/weixin_42010722/article/details/120199407#_1)

## [Conv2d前向与反向](https://blog.csdn.net/weixin_42010722/article/details/120199407#Conv2d_2)

## [MaxPool2d](https://blog.csdn.net/weixin_42010722/article/details/120199407#MaxPool2d_74)

## [BatchNorm2d](https://blog.csdn.net/weixin_42010722/article/details/120199407#BatchNorm2d_133)

## [Flatten层](https://blog.csdn.net/weixin_42010722/article/details/120199407#Flatten_209)

## [全连接层的前向与反向](https://blog.csdn.net/weixin_42010722/article/details/120199407#_233)

## [Dropout前向与反向](https://blog.csdn.net/weixin_42010722/article/details/120199407#Dropout_286)

## [激活函数](https://blog.csdn.net/weixin_42010722/article/details/120199407#_321)

## [ReLU](https://blog.csdn.net/weixin_42010722/article/details/120199407#ReLU_322)

## [Tanh](https://blog.csdn.net/weixin_42010722/article/details/120199407#Tanh_347)

## [Sigmoid](https://blog.csdn.net/weixin_42010722/article/details/120199407#Sigmoid_365)

## [损失函数](https://blog.csdn.net/weixin_42010722/article/details/120199407#_386)

## [优化器](https://blog.csdn.net/weixin_42010722/article/details/120199407#_456)

## [SGD](https://blog.csdn.net/weixin_42010722/article/details/120199407#SGD_457)